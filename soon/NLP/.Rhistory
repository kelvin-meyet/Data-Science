ILPD <-kNN(ILPD, variable=c("agratio"), k=10)
ILPD <- subset(ILPD, select=age:liver) #removing the defunct imputed column
summary(ILPD)
table(is.na(ILPD))
str(ILPD)
tbl <- table(ILPD$gender, ILPD$liver); tbl
tbl1<- table(ILPD$liver);tbl1
# 1 coded as "yes", 2 coded as "no"
# qqnorm(ILPD$liver, main="Q-Q Plot of patients with liver diease")
# qqline(tbl1)
colors =c("darkred","darkblue")
barplot(tbl,xlab="Liver Diagnosis(1=YES:2 = N0)",ylab="Counts of LiverDiagnosis",
main="Barplot of Patients with Liver Disease" , col=colors, beside=TRUE,
ylim=c(0,350))
box()
legend("topright", fill=colors, legend=c("FEMALE", "MALE"))
plot(ILPD$TB,ILPD$liver, pch=19, col="brown", xlab= "TB", ylab="Liver Diagnosis",
main="Plot of Liver Diagnosis vrs TB counts")
knitr::opts_chunk$set(echo = TRUE)
install.packages("kernlab")
#install.packages("kernlab")
require(kernlab)
data(spam)
#install.packages("kernlab")
require(kernlab)
data(spam)
View(spam)
#install.packages("kernlab")
require(kernlab)
data(spam)
View(spam)
table(is.na(spam))
#1.Data preparation
#a
library(kernlab)
data(spam)
mydata<-(spam)
names(mydata)
dim(mydata)
miss.info <- function(mydata, filename=NULL){
vnames <- colnames(mydata)
n <- nrow(mydata)
out <- NULL
for (j in 1: ncol(mydata)){
vname <- colnames(mydata)[j]
x <- as.vector(mydata[,j])
n1 <- sum(is.na(x), na.rm=T)
n2 <- sum(x=="NA", na.rm=T)
n3 <- sum(x=="", na.rm=T)
nmiss <- n1 + n2 + n3
ncomplete <- n-nmiss
out <- rbind(out, c(col.number=j, vname=vname,
mode=mode(x), n.levels=length(unique(x)),
ncomplete=ncomplete, miss.perc=nmiss/n))
}
out <- as.data.frame(out)
row.names(out) <- NULL
return(out)
}
miss.info(mydata)
#b Exploratory data analysis
x<-table(spam$type)
prop.table(x)*100
str(mydata)
zero.info <- function(mydata, filename=NULL){
vnames <- colnames(mydata)
n <- nrow(mydata)
out <- NULL
for (j in 1: ncol(mydata)){
vname <- colnames(mydata)[j]
x <- as.vector(mydata[,j])
n1<- sum(x==0)
zero<- n1
out <- rbind(out, c(col.number=j, vname=vname,
zero.perc=zero/n))
}
out <- as.data.frame(out)
row.names(out) <- NULL
return(out)
}
zero.info(mydata)
library("dplyr")
library("ggpubr")
ggdensity(mydata$you)
ggqqplot(mydata$you)
ggdensity(mydata$your)
ggqqplot(mydata$your)
ggdensity(mydata$charExclamation)
ggqqplot(mydata$charExclamation)
ggdensity(mydata$capitalAve)
ggqqplot(mydata$capitalAve )
#c Data Partitioning
mydata$type<- ifelse(mydata$type=="spam", 1, 0)
table(mydata$type)
n <- NROW(mydata); ratio <- 2/3
set.seed(123)
training <- sample(1:n, size=trunc(n*ratio), replace=FALSE)
dat.training <- mydata[training, ]
dat.test <- mydata[-training, ]
dim(dat.training)
dim(dat.test)
###2. Supervised learning
#Linear discriminant analysis (LDA)
library(MASS)
formula0<-type~.
fit.LDA <- lda(formula0, data=dat.training)
yhat.LDA <- predict(fit.LDA, newdata=dat.test)$x
## LOGISTIC WITH l2 PENALTY
library(glmnet)
lambda <- seq(0, 10.0, 0.01)
X <- model.matrix(object=formula0, data=dat.training); dim(X)
cv.RR <- cv.glmnet(x=X, y=dat.training$type, family="binomial",alpha = 0,lambda = lambda,nfolds=10)
plot(cv.RR)
lmbd0 <- cv.RR$lambda.min; lmbd0
fit.logitRR <- cv.RR$glmnet.fit
X.test <- model.matrix(object=formula0, data=dat.test);
yhat.logitRR <- predict(fit.logitRR, s=lmbd0, newx=X.test, type="response")
## One single decision tree
library(rpart)
control0 <- rpart.control(minsplit=10, minbucket=3, maxdepth=10,
cp=0, maxcompete=4,
maxsurrogate=5, usesurrogate=2, surrogatestyle=0,
xval=10)
tre0 <- rpart(type ~. , data=dat.training,  method="class", control=control0,
parms=list(split="gini"))
plot(tre0); text(tre0)
cv.error <- (tre0$cptable)[,4]
a0 <- 1
SE1 <- min(cv.error) + a0*((tre0$cptable)[,5])[which.min(cv.error)]
position <- min((1:length(cv.error))[cv.error <= SE1])
n.size  <- (tre0$cptable)[,2] + 1
best.size.1SE <- n.size[position]; best.size.1SE
best.cp <-  sqrt(prod(tre0$cptable[(position-1):position,1]))
best.cp
best.tree <- prune(tre0, cp=best.cp)
summary(best.tree)
library(rpart.plot)
yhat.tree<- predict(best.tree, newdata = dat.test, type="vector")
#TRAINING ERROR
btre <- prune(tre0, cp=0.002978265)
btre.train.class <- predict(btre, type='class')
table(predicted=btre.train.class, actual=dat.training$type)
# TEST ERROR
btre.test.class <- predict(btre, type='class', newdata=dat.test)
table(predicted=btre.test.class, actual=dat.test$type)
#Bagging
library(ipred)
library(mlbench)
#Random Forest
library(randomForest)
install.packages("randomForest")
install.packages("mlbench")
#Bagging
library(ipred)
library(mlbench)
fit.bagging <- bagging(type~., data=dat.training, nbagg=50, coob=TRUE)
print(fit.bagging)
summary(fit.bagging)
yhat.bag <- predict(fit.bagging, newdata=dat.test, type="prob")
#Random Forest
library(randomForest)
set.seed(123)
m.try <- tuneRF(dat.training[ , -c(58)], dat.training[,58], ntreeTry=50, stepFactor=2,
improve=0.05, trace=TRUE, plot=TRUE, dobest=FALSE)
best.m <- m.try[m.try[, 2] == min(m.try[, 2]), 1]; best.m
fit.rf <- randomForest(type ~.,data=dat.training, importance=TRUE, proximity=TRUE,ntree=500)
data(spam)
data(spam)
knitr::opts_chunk$set(echo = TRUE)
#install.packages("kernlab")
require(kernlab)
data(spam)
mydata <- spam
dim(mydata)
View(spam)
View(spam)
?bagging
?relief
#install.packages("ICSOutlier")
suppressPackageStartupMessages(library("ICSOutlier"))
install.packages("ICSOutlier")
suppressPackageStartupMessages(library("ICSOutlier"))
#library("ICSOutlier")
data(HTP)
dat <- HTP; dim(dat);
#head(dat)
outliers.true <- c(581, 619)
install.packages("ICSOutlier")
suppressPackageStartupMessages(library("ICSOutlier"))
#library("ICSOutlier")
data(HTP)
dat <- HTP; dim(dat);
#head(dat)
outliers.true <- c(581, 619)
#install.packages("ICSOutlier")
suppressPackageStartupMessages(library("ICSOutlier"))
#library("ICSOutlier")
data(HTP)
dat <- HTP; dim(dat);
#head(dat)
outliers.true <- c(581, 619)
View(HTP)
ics2(HTP)
w=ics2(HTP)
View(w)
icsHTP <- ics2(HTP)
icsOutlierDA <- ics.outlier(icsHTP, test = "agostino.test", level.test = 0.05,
level.dist = 0.02, mDist = 50, ncores = 1)
plot(icsOutlierDA)
View(dat)
#install.packages("ICSOutlier")
suppressPackageStartupMessages(library("ICSOutlier"))
#library("ICSOutlier")
data(HTP)
dat <- HTP; dim(dat); #unlabelled data
outliers.true <- c(581, 619) # index of two defective products returned by customer.
#First obtain robust estimates of the mean vector  and
#VCOV matrix of the data with MCD with a breakdown point of your choice
#install.packages("robustbase")
library(robustbase)
# Obtain MCD estimates with a breakdown point of 30%
fit.robust <- covMcd(dat, cor = FALSE, alpha = 0.70)
fit.robust$center #robust estimates of mean vector
fit.robust$COV #robust estimates of variance-covariance matrix
fit.robust$COV
#First obtain robust estimates of the mean vector  and
#VCOV matrix of the data with MCD with a breakdown point of your choice
#install.packages("robustbase")
library(robustbase)
# Obtain MCD estimates with a breakdown point of 30%
fit.robust <- covMcd(dat, cor = FALSE, alpha = 0.70)
'fit.robust$center' #robust estimates of mean vector
'fit.robust$COV' #robust estimates of variance-covariance matrix
#Compute the robust Mahalanobis distance of each observation with respect to the MCD estimates and plot them.
RD <- mahalanobis(dat, fit.robust$center, fit.robust$cov)
RD[1:30]
# Cut-off based on the chi-square distribution
cutoff.chi.sq <- qchisq(0.975, df = ncol(dat)); cutoff.chi.sq
#All observations above this cutoff value are potential outliers.
# Cut-off based on the chi-square distribution
cutoff.chi.sq <- qchisq(0.975, df = ncol(dat)); cutoff.chi.sq
#All observations above this cutoff value are potential outliers.
# PLOT THE RESULTS
colPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 1, grey(0.5))
pchPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 16, 4)
plot(seq_along(RD), RD, pch = pchPoints, col = colPoints,
ylim=c(0, max(RD, cutoff.chi.sq) + 2), cex.axis = 0.7, cex.lab = 0.7,
ylab = expression(RD**2), xlab = "Observation Number")
abline(h = c(cutoff.chi.sq), lty = c("dashed", "dotted"), col=c("blue", "red"))
legend("topleft", lty = c("dashed", "dotted"), cex = 0.7, ncol = 2, bty = "n",
legend = c(expression(paste(chi[p]**2, " cut-off"))), col=c("blue", "red"))
text(619, RD[619], labels = 619, col=619)
text(581, RD[581], labels = 581, col=581)
which(RD >= cutoff.chi.sq) #outlier IDS - ALL POTENTIAL OUTLIERS
#inspect the most outlying observation
#Top list of potential outliers would actually be above an RD(Mahalanobis value of 22500)
most.outly<- which(RD > 22500)
most.outly
# PLOT THE RESULTS
colPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 1, grey(0.5))
pchPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 16, 4)
plot(seq_along(RD), RD, pch = pchPoints, col = colPoints,
ylim=c(0, max(RD, cutoff.chi.sq) + 2), cex.axis = 0.7, cex.lab = 0.7,
ylab = expression(RD**2), xlab = "Observation Number")
abline(h = c(cutoff.chi.sq), lty = c("dashed", "dotted"), col=c("yellow", "red"))
legend("topleft", lty = c("dashed", "dotted"), cex = 0.7, ncol = 2, bty = "n",
legend = c(expression(paste(chi[p]**2, " cut-off"))), col=c("blue", "red"))
text(619, RD[619], labels = 619, col=619)
text(581, RD[581], labels = 581, col=581)
# PLOT THE RESULTS
colPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 1, grey(0.5))
pchPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 16, 4)
plot(seq_along(RD), RD, pch = pchPoints, col = colPoints,
ylim=c(0, max(RD, cutoff.chi.sq) + 2), cex.axis = 0.7, cex.lab = 0.7,
ylab = expression(RD**2), xlab = "Observation Number")
abline(h = c(cutoff.chi.sq), lty = c("dashed", "dotted"),     )
legend("topleft", lty = c("dashed", "dotted"), cex = 0.7, ncol = 2, bty = "n",
legend = c(expression(paste(chi[p]**2, " cut-off"))),    )
text(619, RD[619], labels = 619, col=619)
text(581, RD[581], labels = 581, col=581)
# PLOT THE RESULTS
colPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 1, grey(0.5))
pchPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 16, 4)
plot(seq_along(RD), RD, pch = pchPoints, col = colPoints,
ylim=c(0, max(RD, cutoff.chi.sq) + 2), cex.axis = 0.7, cex.lab = 0.7,
ylab = expression(RD**2), xlab = "Observation Number")
abline(h = c(cutoff.chi.sq), lty = c("dashed", "dotted"),     ) # col=c("blue", "red")
legend("topleft", lty = c("dashed", "dotted"), cex = 0.7, ncol = 2, bty = "n",
legend = c(expression(paste(chi[p]**2, " cut-off"))),    ) # col=c("blue", "red")
text(619, RD[619], labels = 619, col=619)
text(581, RD[581], labels = 581, col=581)
# PLOT THE RESULTS
colPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 1, grey(0.5))
pchPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 16, 4)
plot(seq_along(RD), RD, pch = pchPoints, col = colPoints,
ylim=c(0, max(RD, cutoff.chi.sq) + 2), cex.axis = 0.7, cex.lab = 0.7,
ylab = expression(RD**2), xlab = "Observation Index", main="k")
abline(h = c(cutoff.chi.sq), lty = c("dashed", "dotted"),     ) # col=c("blue", "red")
legend("topleft", lty = c("dashed", "dotted"), cex = 0.7, ncol = 2, bty = "n",
legend = c(expression(paste(chi[p]**2, " cut-off"))),    ) # col=c("blue", "red")
text(619, RD[619], labels = 619, col=619)
text(581, RD[581], labels = 581, col=581)
# PLOT THE RESULTS
colPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 1, grey(0.5))
pchPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 16, 4)
plot(seq_along(RD), RD, pch = pchPoints, col = colPoints,
ylim=c(0, max(RD, cutoff.chi.sq) + 2), cex.axis = 0.7, cex.lab = 0.7,
ylab = expression(RD**2), xlab = "Observation Index", main="Outlier Detection with MCD")
abline(h = c(cutoff.chi.sq), lty = c("dashed", "dotted"),     ) # col=c("blue", "red")
legend("topleft", lty = c("dashed", "dotted"), cex = 0.7, ncol = 2, bty = "n",
legend = c(expression(paste(chi[p]**2, " cut-off"))),    ) # col=c("blue", "red")
text(619, RD[619], labels = 619, col=619)
text(581, RD[581], labels = 581, col=581)
#ISOLATION FOREST
suppressPackageStartupMessages(library("devtools"))
#devtools::install_github("Zelazny7/isofor")
suppressPackageStartupMessages(library(isofor))
devtools::install_github("Zelazny7/isofor")
#ISOLATION FOREST
suppressPackageStartupMessages(library("devtools"))
#devtools::install_github("Zelazny7/isofor")
suppressPackageStartupMessages(library(isofor))
# help(package="isofor")
# An isolation forest model with 200 trees and 256 samples drawn
#to construct each tree(phi)
fit.isoforest <- iForest(dat, nt=200, phi=256)
pred <- predict(fit.isoforest, newdata=dat)
#pred
# PLOT OF THE SCORES
score <- scale(pred, center = min(pred), scale = max(pred)-min(pred))
par(mfrow=c(1,1), mar=rep(4,4))
plot(x=1:length(score), score, type="p", pch=1,
main="Anomaly Score via iForest",
xlab="id", ylab="score", cex=score*3, col="darkviolet")
add.seg <- function(x) segments(x0=x[1], y0=0, x1=x[1], y1=x[2],
lty=1, lwd=1.5, col="forestgreen")
apply(data.frame(id=1:length(score), score=score), 1, FUN=add.seg)
eps <- 0.99
id.outliers <- which(score > quantile(score, eps))
text(id.outliers, score[id.outliers]+0.005, label=id.outliers,
col="red", cex=0.7)
id.outliers
# PLOT OF THE SCORES
score <- scale(pred, center = min(pred), scale = max(pred)-min(pred))
par(mfrow=c(1,1), mar=rep(4,4))
plot(x=1:length(score), score, type="p", pch=1,
main="Anomaly Score via Isoforest",
xlab="id", ylab="score", cex=score*3, col="darkviolet")
add.seg <- function(x) segments(x0=x[1], y0=0, x1=x[1], y1=x[2],
lty=1, lwd=1.5, col="forestgreen")
apply(data.frame(id=1:length(score), score=score), 1, FUN=add.seg)
eps <- 0.99
id.outliers <- which(score > quantile(score, eps));id.outliers
text(id.outliers, score[id.outliers]+0.005, label=id.outliers,
col="red", cex=0.7)
id.outliers
# LOCAL OUTLIER FACTOR
#install.packages("Rlof")
#help(package="Rlof")
library(Rlof)
#Obtained the LOF of our dataset with k-nearest neighbour, which considers the density of the neighbourhood(k) around the observation to determineits outliers, in this case k=10.
outlier.scores <- lof(dat, k=10);  outlier.scores [1:20]
which(outlier.scores > quantile(outlier.scores, 0.95))
# LOCAL OUTLIER FACTOR
#install.packages("Rlof")
#help(package="Rlof")
library(Rlof)
#Obtained the LOF of our dataset with k-nearest neighbour, which considers the density of the neighbourhood(k) around the observation to determineits outliers, in this case k=10.
outlier.scores <- lof(dat, k=10);  outlier.scores [1:20]
which(outlier.scores > quantile(outlier.scores, 0.95))
# PLOT OF THE LOF SCORES
score <- scale(outlier.scores, center = min(outlier.scores),
scale = max(outlier.scores)-min(outlier.scores)) # NORMALIZED TO RANGE[0,1]
par(mfrow=c(1,1), mar=rep(4,4))
plot(x=1:length(score), score, type="p", pch=1,
main="Local Outlier Factor (LOF)",
xlab="id", ylab="LOF", cex=score*5, col="darkviolet")
add.seg <- function(x) segments(x0=x[1], y0=0, x1=x[1], y1=x[2],
lty=1, lwd=1.5, col="forestgreen")
apply(data.frame(id=1:length(score), score=score), 1, FUN=add.seg)
eps <- 0.99
id.outliers <- which(outlier.scores > quantile(outlier.scores, eps))
text(id.outliers, score[id.outliers]+0.005, label=id.outliers,
col="red", cex=0.7)
#install.packages("ICSOutlier")
suppressPackageStartupMessages(library("ICSOutlier"))
#library("ICSOutlier")
data(HTP)
dat <- HTP; dim(dat); #labelled data: 88 features & 902 observations
outliers.true <- c(581, 619) # index of two defective products returned by customer.
#First obtain robust estimates of the mean vector  and
#VCOV matrix of the data with MCD with a breakdown point of your choice
#install.packages("robustbase")
library(robustbase)
# Obtain MCD estimates with a breakdown point of 30%
fit.robust <- covMcd(dat, cor = FALSE, alpha = 0.70)
'fit.robust$center' #robust estimates of mean vector
'fit.robust$COV' #robust estimates of variance-covariance matrix
#Compute the robust Mahalanobis distance of each observation with respect to the MCD estimates and plot them.
RD <- mahalanobis(dat, fit.robust$center, fit.robust$cov)
RD[1:30]
# Cut-off based on the chi-square distribution
cutoff.chi.sq <- qchisq(0.975, df = ncol(dat)); cutoff.chi.sq
#All observations above this cutoff value are potential outliers.
# PLOT THE RESULTS
colPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 1, grey(0.5))
pchPoints <- ifelse(RD >= min(c(cutoff.chi.sq)), 16, 4)
plot(seq_along(RD), RD, pch = pchPoints, col = colPoints,
ylim=c(0, max(RD, cutoff.chi.sq) + 2), cex.axis = 0.7, cex.lab = 0.7,
ylab = expression(RD**2), xlab = "Observation Index", main="Outlier Detection with MCD")
abline(h = c(cutoff.chi.sq), lty = c("dashed", "dotted"),     ) # col=c("blue", "red")
legend("topleft", lty = c("dashed", "dotted"), cex = 0.7, ncol = 2, bty = "n",
legend = c(expression(paste(chi[p]**2, " cut-off"))),    ) # col=c("blue", "red")
text(619, RD[619], labels = 619, col=619)
text(581, RD[581], labels = 581, col=581)
which(RD >= cutoff.chi.sq) #outlier IDS - ALL POTENTIAL OUTLIERS
#inspect the most outlying observation
#Top list of potential outliers would actually be above an RD(Mahalanobis value of 22500)
most.outly<- which(RD > 22500)
most.outly
#ISOLATION FOREST
suppressPackageStartupMessages(library("devtools"))
#devtools::install_github("Zelazny7/isofor")
suppressPackageStartupMessages(library(isofor))
# help(package="isofor")
# An isolation forest model with 200 trees and 256 samples drawn  to construct each tree(phi)
fit.isoforest <- iForest(dat, nt=200, phi=256)
pred <- predict(fit.isoforest, newdata=dat)#pred
# PLOT OF THE SCORES
score <- scale(pred, center = min(pred), scale = max(pred)-min(pred))
par(mfrow=c(1,1), mar=rep(4,4))
plot(x=1:length(score), score, type="p", pch=1,
main="Anomaly Score via Isoforest",
xlab="id", ylab="score", cex=score*3, col="darkviolet")
add.seg <- function(x) segments(x0=x[1], y0=0, x1=x[1], y1=x[2],
lty=1, lwd=1.5, col="forestgreen")
apply(data.frame(id=1:length(score), score=score), 1, FUN=add.seg)
eps <- 0.99
id.outliers <- which(score > quantile(score, eps));id.outliers
text(id.outliers, score[id.outliers]+0.005, label=id.outliers,
col="red", cex=0.7)
# LOCAL OUTLIER FACTOR
#install.packages("Rlof")
#help(package="Rlof")
library(Rlof)
#Obtained the LOF of our dataset with k-nearest neighbour, which considers the density of the neighbourhood(k) around the observation to determineits outliers, in this case k=10.
outlier.scores <- lof(dat, k=10);  outlier.scores [1:20]
which(outlier.scores > quantile(outlier.scores, 0.95))
# PLOT OF THE LOF SCORES
score <- scale(outlier.scores, center = min(outlier.scores),
scale = max(outlier.scores)-min(outlier.scores)) # NORMALIZED TO RANGE[0,1]
par(mfrow=c(1,1), mar=rep(4,4))
plot(x=1:length(score), score, type="p", pch=1,
main="Local Outlier Factor (LOF)",
xlab="id", ylab="LOF", cex=score*5, col="darkviolet")
add.seg <- function(x) segments(x0=x[1], y0=0, x1=x[1], y1=x[2],
lty=1, lwd=1.5, col="forestgreen")
apply(data.frame(id=1:length(score), score=score), 1, FUN=add.seg)
eps <- 0.99
id.outliers <- which(outlier.scores > quantile(outlier.scores, eps))
text(id.outliers, score[id.outliers]+0.005, label=id.outliers,
col="red", cex=0.7)
getwd()
# Importing the dataset
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
getwd()
setwd("D:/school/PhD/summer 2022/ML projects/ml_projects/NLP")
# Importing the dataset
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
# Cleaning the texts
# install.packages('tm')
# install.packages('SnowballC')
library(tm)
library(SnowballC)
?tm_map()
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
# Creating the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset$Liked = dataset_original$Liked
# Encoding the target feature as factor
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Fitting Random Forest Classification to the Training set
# install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-692])
# Making the Confusion Matrix
cm = table(test_set[, 692], y_pred)
cm
