---
title: "Random Forest & Decision Trees"
author: "KELVIN OFORI-MINTA"
#date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    number_sections: yes
    toc: yes
    toc_depth: 4
  word_document:
    toc: yes
    toc_depth: '4'
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsfonts}
- \usepackage{amsthm}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhf{}
- \rhead{Kelvin}
- \lhead{Anomaly Detection Techniques}
- \cfoot{\thepage}
- \usepackage{algorithm}
- \usepackage[noend]{algpseudocode}
geometry: margin=1in
spacing: single
fontsize: 12pt
---

## QUESTION 1 - LOAD DATA AND FAMILIARISE WITH VARIABLES  
```{r}
#install.packages("kernlab")
require(kernlab)
data(spam)
mydata <- spam
dim(mydata)
```
The data has 4601 rows and 58 columns. 

```{r}
#inspect missing data
table(is.na(spam))
```
There are no missing values in the 266858 cells.


```{r}

names(mydata)
dim(mydata)

```

The entire dataset have 4601 observations and 58 variables.  

```{r}

miss.rate <- sapply(mydata, function(x)sum(is.na(x))/4601)

a<- paste(round((miss.rate)*100,digits=2),"%",sep="")
b<- colnames(mydata)
missrate <- structure(a,names=b)
print(miss.rate)
```
This output indicates there are no missing values


```{r}
#b Exploratory data analysis 
x<-table(spam$type)
prop.table(x)*100

table(mydata$type)
```
This shows that 60.6% are non spam mails whilst 39.40% are spam mails.
zero was coded as non-spam & 1 coded as spam
\newpage
```{r}
str(mydata)
```
All predictors seem to be measured on a continuous scale, with the dependent variable being categorical.
Most entries seem to be zero for some selected variables

\newpage
```{r}
zero.info <- function(mydata, filename=NULL){
vnames <- colnames(mydata)
n <- nrow(mydata)
out <- NULL
for (j in 1: ncol(mydata)){
vname <- colnames(mydata)[j]
x <- as.vector(mydata[,j])
n1<- sum(x==0)
zero<- n1
out <- rbind(out, c(col.number=j, vname=vname,
zero.perc=zero/n))
}
out <- as.data.frame(out)
row.names(out) <- NULL
return(out)
}
zero.info(mydata)
```
Displays the percentage of zero entries in each variable.

\newpage
```{r}
library("dplyr")
library("ggpubr")

par(mfrow=c(2,2))

ggqqplot(mydata$you)

ggqqplot(mydata$your)

ggqqplot(mydata$charExclamation)

ggqqplot(mydata$capitalAve )
```
The qq plot of the selected variables above shows deviation from the assumption of normality.

```{r}
#c Data Partitioning
mydata$type<- ifelse(mydata$type=="spam", 1, 0)
table(mydata$type)
n <- NROW(mydata); ratio <- 2/3
set.seed(123)
training <- sample(1:n, size=trunc(n*ratio), replace=FALSE)
train <- mydata[training, ]
test <- mydata[-training, ]
dim(train)
dim(test)
```
Train set is has a dimension of 3067 rows of observations and 58 columns of variables.
Test set has a dimension of 1534 observational rows and 58 columns of variables.


\newpage
## QUESTION 2 - SUPERVISED LEARNING.  

# (a) Linear Discriminat Analysis  (LDA)
```{r}

library(MASS)
formula0<-type~.
fit.LDA <- lda(formula0, data=train)
yhat.LDA <- predict(fit.LDA, newdata=test)$x
fit.LDA
```
Displays coefficients of linear discriminants stored in fit.LDA
The predicted values also stored in yhat.LDA


# LOGISTIC REGRESSION WITH L2 PENALTY.  

```{r}
## LOGISTIC WITH l2 PENALTY

library(glmnet)
lambda <- seq(0, 10.0, 0.01)
X <- model.matrix(object=formula0, data=train)
cv.RR <- cv.glmnet(x=X, y=train$type, family="binomial",alpha = 0,lambda = lambda,nfolds=10)
plot(cv.RR)
lmbd0 <- cv.RR$lambda.min; lmbd0
fit.logitRR <- cv.RR$glmnet.fit
X.test <- model.matrix(object=formula0, data=test);
yhat.logitRR <- predict(fit.logitRR, s=lmbd0, newx=X.test, type="response")
```

A lambda value of zero was obtained.

\newpage
# One single decision tree.  
```{r}
## One single decision tree
library(rpart)
control0 <- rpart.control(minsplit=10, minbucket=3, maxdepth=10,
        cp=0, maxcompete=4, 
        maxsurrogate=5, usesurrogate=2, surrogatestyle=0, 
        xval=10)				
tre0 <- rpart(type ~. , data=train,  method="class", control=control0,
            	parms=list(split="gini"))
plot(tre0); text(tre0)
```
This diagram above shows the decision tree of the spam mailing system.
The lower part of the tree seem almost impossible to read, but further analysis for the best tree will make the readings quite clearer. 
But we can see that;
when the charDollar is < 0.055 and hp >=0.4 and etc a decision will be arrived else when remove < 0.055 and char exclamation is <0.4595 etc, an alternate decision is arrived at 

```{r}
cv.error <- (tre0$cptable)[,4]
a0 <- 1   
SE1 <- min(cv.error) + a0*((tre0$cptable)[,5])[which.min(cv.error)]   
position <- min((1:length(cv.error))[cv.error <= SE1])
n.size  <- (tre0$cptable)[,2] + 1 
best.size.1SE <- n.size[position]; best.size.1SE
best.cp <-  sqrt(prod(tre0$cptable[(position-1):position,1]))
best.cp
best.tree <- prune(tre0, cp=best.cp)
```

\newpage
```{r}
summary(best.tree)
```

\newpage
```{r}
#install.packages("rpart.plot")
library(rpart.plot)
library(RColorBrewer)
?rpart.plot
```

```{r}
rpart.plot(best.tree,  shadow.col="red",
      main="A Tree Model for spam email")

prp(best.tree)
prp(best.tree, main="Decision Tree Model for spam email",
	type=4, box.palette="auto",
    	faclen=0)
```
The diagrams above shows the diagram of the best tree i tried implementing colors to make it readable.
We can now confidently infer from the tree diagram above that;
when charDoll >= 0.056, hp>=0.4, email< 0.28 we can obtain both spam and not spam mails, else when charDoll < 0.056, remove < 0.055, charExcel<0.46, num000<0.34, we obtain a spam mail if free<0.2.

  
\newpage
```{r}
yhat.tree<- predict(best.tree, newdata = test, type="vector")
#TRAINING ERROR
btre <- prune(tre0, cp=0.002978265)
btre.train.class <- predict(btre, type='class')
table(predicted=btre.train.class, actual=train$type)
```


```{r}
# TEST ERROR
btre.test.class <- predict(btre, type='class', newdata=test)
table(predicted=btre.test.class, actual=test$type)

```

## BAGGING.  
```{r}
#Bagging
library(ipred)
library(mlbench)
fit.bagging <- bagging(type~., data=train, nbagg=50, coob=TRUE)
print(fit.bagging)
summary(fit.bagging) 
yhat.bag <- predict(fit.bagging, newdata=test, type="prob")
```

\newpage
##  RANDOM FOREST  

```{r}
#Random Forest
library(randomForest)
set.seed(123)
m.try <- tuneRF(train[ , -c(58)], train[,58], ntreeTry=50, stepFactor=2, 
    improve=0.05, trace=TRUE, plot=TRUE, dobest=FALSE)
best.m <- m.try[m.try[, 2] == min(m.try[, 2]), 1]; best.m
fit.rf <- randomForest(type ~.,data=train, importance=TRUE, proximity=TRUE,ntree=500)
```

 
```{r}
yhat.ran<- predict(fit.rf, newdata=test, type="response")
head(yhat.ran)
```

\newpage
## Boosting  
```{r}
## Boosting
#install.packages("ada")

library(ada)
stump <- rpart.control(cp=-1, maxdepth=1, minsplit=0)

# DISCRETE ADABOOST
fit.stump <- ada(type~., data=train, iter=2000, loss="e", type="discrete", control=stump);
fit.stump
```

```{r}
yhat.boost <- predict(fit.stump, newdata=test, type="probs")[, 2]
```

\newpage
```{r}
#install.packages("cvAUC")
library(cvAUC)
n <- NROW(test)
yobs<-test$type
AUC.LDA <- ci.cvAUC(predictions=yhat.LDA, labels=yobs, folds=1:n, confidence=0.95); AUC.LDA
AUC.logitRR  <- ci.cvAUC(predictions=yhat.logitRR, labels=yobs, folds=1:n, confidence=0.95); AUC.logitRR
cbind(AUC.LDA$cvAUC, AUC.logitRR$cvAUC)
```


```{r}
## Area under the ROC curve
#install.packages("verification")
library(verification)
# LDA
yhat <- scale(yhat.LDA, center=min(yhat.LDA), scale = max(yhat.LDA)-min(yhat.LDA))
mod.glm <- verify(obs=yobs, pred=yhat)
roc.plot(mod.glm, plot.thres = NULL, main="ROC Curve LDA")
text(x=0.5, y=0.2, paste("AUC =", round(AUC.LDA$cvAUC, digits=3), 
	"with 95% CI (", round(AUC.LDA$ci, digits=3)[1], ",", round(AUC.LDA$ci, digits=3)[2], ").",
	sep=" "), col="blue", cex=1.2)


# LOGISTIC WITH l2 REGULARIZATION
mod.glm <- verify(obs=yobs, pred=yhat.logitRR)
roc.plot(mod.glm, plot.thres = NULL, main="ROC Curve Logistic")
text(x=0.5, y=0.2, paste("AUC =", round(AUC.logitRR$cvAUC, digits=3), 
	"with 95% CI (", round(AUC.logitRR$ci, digits=3)[1], ",", round(AUC.logitRR$ci, digits=3)[2], ").",
	sep=" "), col="blue", cex=1.2, main="LDA")


#One single decision tree
y<-test$type
a.ROC <- roc.area(obs=y, pred=yhat.tree)$A; 
AUC.tree <- round(a.ROC, digits=4); AUC.tree
mod.glm <- verify(obs=y, pred=yhat.tree, bins = FALSE)
roc.plot(mod.glm, plot.thres = NULL,col="red", main ="ROC Curve tree model")
text(x=0.6, y=0.3, paste("Area under ROC = ", AUC.tree, sep="")) 

#bagging
a.ROC <- roc.area(obs=y, pred=yhat.bag)$A
AUC.bag <- round(a.ROC, digits=4)
mod.glm <- verify(obs=yobs, pred=yhat.bag)
roc.plot(mod.glm, plot.thres = NULL, col="red",main ="ROC Curve bagging")
text(x=0.7, y=0.2, paste("Area under ROC =", round(AUC.bag, digits=4), 
	sep=" "), col="blue", cex=1.2)

#Random Forest
a.ROC <- roc.area(obs=y, pred=yhat.ran)$A
AUC.ran <- round(a.ROC, digits=4)
mod.glm <- verify(obs=y, pred=yhat.ran)
roc.plot(mod.glm, plot.thres = NULL, col="red",main ="ROC Curve ran.forest")
text(x=0.7, y=0.2, paste("Area under ROC =", round(AUC.ran, digits=4), 
	sep=" "), col="blue", cex=1.2)
#boosting
a.ROC<- roc.area(obs=y, pred=yhat.boost)$A
AUC.boost<-round(a.ROC, digits=4)
mod.glm <- verify(yobs, pred=yhat.boost)
roc.plot(mod.glm, plot.thres = NULL, col="red",main ="ROC Curve boosting")
text(x=0.7, y=0.2, paste("Area under ROC =", round(AUC.boost, digits=4), 
	sep=" "), col="blue", cex=1.2)

```
All Area under ROC curve obtained proved to be sufficiently suitable fir for prediction since all areas under the ROC were well above 0.9 
 
```{r}
cbind(AUC.LDA$cvAUC, AUC.logitRR$cvAUC,AUC.tree,AUC.bag,AUC.ran,AUC.boost)
```
From this output, we could see that the boosting model provided AUC value of 0.982
hence making it the best model

\newpage
## QUESTION 3. 
# A Random Forest model with B=2000 trees with the entire spam data set.

```{r}
## Random forest
library(randomForest)
set.seed(123)
m.try <- tuneRF(train[ , -c(58)], train[,58], ntreeTry=2000,stepFactor=2, 
                improve=0.05, trace=TRUE, plot=TRUE, dobest=FALSE)
best.m <- m.try[m.try[, 2] == min(m.try[, 2]), 1]; best.m
 
plot(fit.rf, main="Out-of-Bag Estimate of Error")
```

\newpage
# 3(a) Variable Importance ranking plots  
```{r}
# VARIABLE IMPORTANCE RANKING
round(importance(fit.rf), 2)
varImpPlot(fit.rf, main="Variable Importance Ranking") 
```
From this two graph we can conclude that the variables charExclamation,remove ,charDollar,hp,capitalAve,free,capitalLong are important variables. 

# 3(b)- Partial Dependence Plot  of two most important variables.  
```{r}
par(mfrow=c(1,2))
partialPlot(fit.rf, pred.data=train, x.var=charExclamation , rug=TRUE)
partialPlot(fit.rf, pred.data=train, x.var=charDollar, rug=TRUE)
```

# (c) Proximity Matrix.  
```{r}
# 3.(PROXIMITY MATRIX
fit.mds <- cmdscale(1 - fit.rf$proximity, eig=TRUE)
Labels <- train$type
plot(fit.mds$points, xlab="dim I", ylab="dim II", pch=as.numeric(train$type),
     col=c("red","green")[as.numeric(train$type)])
```
The plot showing red indicates that spam mails. 


